{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string \n",
    "import preprocessor as p \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "\n",
    "with open('./sts-2017-en-es/En_Es_STS/STS.input.en-es.train.txt', 'r') as inFile:\n",
    "    input_data = inFile.readlines()\n",
    "\n",
    "X = []\n",
    "for data in input_data:\n",
    "    X.append(data.split('\\t')[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[x[0],x[1]] for x in X]\n",
    "\n",
    "cleaned_eng = []\n",
    "cleaned_esp = []\n",
    "for x in X:\n",
    "    eng, esp = x[0], x[1]\n",
    "    eng, esp = p.clean(eng).lower(), p.clean(esp).lower() #cleaning\n",
    "    for c in string.punctuation: #removing punctuations\n",
    "        if c in eng:\n",
    "            eng = eng.replace(c,'')\n",
    "        if c in esp:\n",
    "            esp = esp.replace(c,'')\n",
    "    \n",
    "    eng, esp = word_tokenize(eng), word_tokenize(esp) #tokenizing\n",
    "    cleaned_eng.append(eng)\n",
    "    cleaned_esp.append(esp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his', 'rheumy', 'eyes', 'began', 'to', 'cloud']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make vocab\n",
    "eng_vocab = {}\n",
    "index=0\n",
    "for sentence in cleaned_eng:\n",
    "    for word in sentence:\n",
    "        if word in eng_vocab:\n",
    "            continue\n",
    "        else:\n",
    "            eng_vocab[word] = index\n",
    "            index+=1\n",
    "\n",
    "esp_vocab = {}\n",
    "index=0\n",
    "for sentence in cleaned_eng:\n",
    "    for word in sentence:\n",
    "        if word in esp_vocab:\n",
    "            continue\n",
    "        else:\n",
    "            esp_vocab[word] = index\n",
    "            index+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f54ec556731191ae34ffecbd12704bb8f2e6a5cabce98c16cdcccd50acdfe5bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
