{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/arihanth.srikar/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['x']\n",
    "y = train_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('/scratch/arihanth.srikar/models/cc.en.300.bin')\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext.util.reduce_model(ft, 100)\n",
    "# ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home2/arihanth.srikar/projects/mynlp/en-en.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=0'>1</a>\u001b[0m final_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m (eng_sent, esp_sent) \u001b[39min\u001b[39;00m X:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=2'>3</a>\u001b[0m     eng_vect \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mtensor([ft\u001b[39m.\u001b[39mget_sentence_vector(word)]) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m eng_sent], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=3'>4</a>\u001b[0m     esp_vect \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mtensor([ft\u001b[39m.\u001b[39mget_sentence_vector(word)]) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m esp_sent], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=4'>5</a>\u001b[0m     final_embeddings\u001b[39m.\u001b[39mappend((eng_vect, esp_vect))\n",
      "\u001b[1;32m/home2/arihanth.srikar/projects/mynlp/en-en.ipynb Cell 7'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=0'>1</a>\u001b[0m final_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m (eng_sent, esp_sent) \u001b[39min\u001b[39;00m X:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=2'>3</a>\u001b[0m     eng_vect \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mtensor([ft\u001b[39m.\u001b[39mget_sentence_vector(word)]) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m eng_sent], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=3'>4</a>\u001b[0m     esp_vect \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mtensor([ft\u001b[39m.\u001b[39mget_sentence_vector(word)]) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m esp_sent], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode81/home2/arihanth.srikar/projects/mynlp/en-en.ipynb#ch0000007vscode-remote?line=4'>5</a>\u001b[0m     final_embeddings\u001b[39m.\u001b[39mappend((eng_vect, esp_vect))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ft' is not defined"
     ]
    }
   ],
   "source": [
    "final_embeddings = []\n",
    "for (eng_sent, esp_sent) in X:\n",
    "    eng_vect = torch.cat([torch.tensor([ft.get_sentence_vector(word)]) for word in eng_sent], dim=0).unsqueeze(dim=1)\n",
    "    esp_vect = torch.cat([torch.tensor([ft.get_sentence_vector(word)]) for word in esp_sent], dim=0).unsqueeze(dim=1)\n",
    "    final_embeddings.append((eng_vect, esp_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedder = WordEmbeddings('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings = []\n",
    "for (e1, e2) in X:\n",
    "    v1 = Sentence(e1)\n",
    "    v2 = Sentence(e2)\n",
    "    v1 = [token.embedding.cpu() for token in glove_embedder.embed(v1)[0]]\n",
    "    v2 = [token.embedding.cpu() for token in glove_embedder.embed(v2)[0]]\n",
    "    final_embeddings.append((v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, sentences, scores):\n",
    "        self.labels = scores\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return self.labels[idx]\n",
    "\n",
    "    def get_batch_embeddings(self, idx):\n",
    "        e1, e2 = self.sentences[idx]\n",
    "        return torch.vstack(e1), torch.vstack(e2)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        embeddings = self.get_batch_embeddings(idx)\n",
    "        label = self.get_batch_labels(idx)\n",
    "\n",
    "        return embeddings, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = Dataset(final_embeddings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2709,  0.0440, -0.0203,  ..., -0.4923,  0.6369,  0.2364],\n",
       "        [ 0.5937,  0.4482,  0.5932,  ..., -0.5465,  0.1516, -0.3075],\n",
       "        [ 0.2616,  0.4472, -0.0968,  ..., -0.4503,  0.4952, -0.2030],\n",
       "        ...,\n",
       "        [ 0.0523,  0.4112, -0.5290,  ..., -0.9391,  0.9866, -0.0713],\n",
       "        [-0.1440,  0.3255,  0.1426,  ...,  0.2540,  1.1078, -0.0731],\n",
       "        [ 0.6848, -0.3764, -0.0787,  ...,  0.4923,  0.1823, -0.0202]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(e1,e2), lbl = next(iter(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim=300, hidden_dim=256, output_size=300):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=self.num_layers, bidirectional=True)\n",
    "\n",
    "    def forward(self, v1):\n",
    "        out, (h, c) = self.lstm(v1.view(v1.shape[0], 1, -1))\n",
    "        out = torch.mean(h.view(h.shape[0], -1), dim=0)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "trainloader, testloader = torch.utils.data.random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(100, 256, 300).to(device)\n",
    "loss_function = F.mse_loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('/scratch/arihanth.srikar/model_saves/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 241.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Train\n",
      "Pearson: 57.3004237612856\n",
      "Loss: 0.8799223235978819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:01<00:00, 744.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Test\n",
      "Pearson: 64.72333552303313\n",
      "Loss 0.7236596801012652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 240.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Train\n",
      "Pearson: 73.50296079079219\n",
      "Loss: 0.5597016836982143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:00<00:00, 767.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Test\n",
      "Pearson: 69.30907354028201\n",
      "Loss 0.6097766486230555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 246.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-Train\n",
      "Pearson: 79.5148208526783\n",
      "Loss: 0.43208297021701747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:01<00:00, 755.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-Test\n",
      "Pearson: 70.91427925315386\n",
      "Loss 0.5701366403293131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 241.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Train\n",
      "Pearson: 83.10494104023749\n",
      "Loss: 0.35593900537114115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:00<00:00, 775.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Test\n",
      "Pearson: 71.80356265045805\n",
      "Loss 0.5495452548515029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 240.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Train\n",
      "Pearson: 85.73029421344643\n",
      "Loss: 0.3005864744952244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:00<00:00, 785.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Test\n",
      "Pearson: 72.40109407124002\n",
      "Loss 0.5351954344893138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 240.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Train\n",
      "Pearson: 87.73543612282991\n",
      "Loss: 0.2583135855423007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:00<00:00, 785.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Test\n",
      "Pearson: 72.84222122325959\n",
      "Loss 0.523507411055253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 240.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6-Train\n",
      "Pearson: 89.36736221093604\n",
      "Loss: 0.22387354234919038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:01<00:00, 750.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6-Test\n",
      "Pearson: 73.19307412325571\n",
      "Loss 0.5145102047839213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 241.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7-Train\n",
      "Pearson: 90.75505086261583\n",
      "Loss: 0.19475192078920087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:01<00:00, 761.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7-Test\n",
      "Pearson: 73.45218644730316\n",
      "Loss 0.5084282413685858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 244.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-Train\n",
      "Pearson: 91.92567330305688\n",
      "Loss: 0.17028243664253412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:00<00:00, 777.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-Test\n",
      "Pearson: 73.69412323863949\n",
      "Loss 0.5035641071931165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 243.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9-Train\n",
      "Pearson: 92.93813281245919\n",
      "Loss: 0.14924011097580495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:01<00:00, 751.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9-Test\n",
      "Pearson: 73.93399840060728\n",
      "Loss 0.499171069495039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 240.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Train\n",
      "Pearson: 93.83855193569826\n",
      "Loss: 0.13060496538788646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:01<00:00, 762.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Test\n",
      "Pearson: 74.08658115365121\n",
      "Loss 0.49630634886570574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 243.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-Train\n",
      "Pearson: 94.6248899452187\n",
      "Loss: 0.11423322921105047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:01<00:00, 764.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-Test\n",
      "Pearson: 74.49344917696538\n",
      "Loss 0.488829005669335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 244.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-Train\n",
      "Pearson: 95.35846714271952\n",
      "Loss: 0.09896007900938229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:00<00:00, 794.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-Test\n",
      "Pearson: 74.79927159584503\n",
      "Loss 0.4819885695894497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 248.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-Train\n",
      "Pearson: 95.94042490432591\n",
      "Loss: 0.08682540422429277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:00<00:00, 767.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-Test\n",
      "Pearson: 74.43574095333216\n",
      "Loss 0.4903849377704773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 240.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14-Train\n",
      "Pearson: 96.31879091073412\n",
      "Loss: 0.07870243201078223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:00<00:00, 770.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14-Test\n",
      "Pearson: 75.16635626470858\n",
      "Loss 0.47466464452382495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "\n",
    "    model.train()\n",
    "    pred, lbls = [], []\n",
    "    running_loss = 0.0\n",
    "    for (e1, e2), lbl in tqdm(trainloader):\n",
    "        model.zero_grad()\n",
    "\n",
    "        e1 = e1.to(device)\n",
    "        e2 = e2.to(device)\n",
    "        lbl = lbl.to(device)\n",
    "\n",
    "        # Run our forward pass.\n",
    "        v1 = model(e1)\n",
    "        v2 = model(e2)\n",
    "\n",
    "        cos = nn.CosineSimilarity(dim=0)\n",
    "        output = ((cos(v1, v2)+1)/2)*5\n",
    "\n",
    "        # Compute the loss, gradients, and update the parameters by\n",
    "        loss = loss_function(output.float(), lbl.float())\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred.append(output.float().item())\n",
    "        lbls.append(lbl.float().item())\n",
    "\n",
    "    print(f'{epoch}-Train')\n",
    "    print('Pearson:', stats.pearsonr(pred, lbls)[0]*100)\n",
    "    print('Loss:', running_loss/len(trainloader))\n",
    "\n",
    "    model.eval()\n",
    "    pred, lbls = [], []\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        for (e1, e2), lbl in tqdm(testloader):\n",
    "            e1 = e1.to(device)\n",
    "            e2 = e2.to(device)\n",
    "            lbl = lbl.to(device)\n",
    "\n",
    "            v1 = model(e1)\n",
    "            v2 = model(e2)\n",
    "\n",
    "            cos = nn.CosineSimilarity(dim=0)\n",
    "            output = ((cos(v1, v2)+1)/2)*5\n",
    "\n",
    "            loss = loss_function(output.float(), lbl.float())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            pred.append(output.float().item())\n",
    "            lbls.append(lbl.float().item())\n",
    "\n",
    "        print(f'{epoch}-Test')\n",
    "        print('Pearson:', stats.pearsonr(pred, lbls)[0]*100)\n",
    "        print('Loss', running_loss/len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/scratch/arihanth.srikar/model_saves/monolingual.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: 60.73496895047812\n"
     ]
    }
   ],
   "source": [
    "pred, lbls = [], []\n",
    "for (e1, e2), lbl in full_dataset:\n",
    "    \n",
    "    v1 = torch.mean(e1.view(e1.shape[0], -1), dim=0)\n",
    "    v2 = torch.mean(e2.view(e2.shape[0], -1), dim=0)\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    output = cos(v1, v2)*5\n",
    "\n",
    "    pred.append(output.float().item())\n",
    "    lbls.append(lbl.float().item())\n",
    "\n",
    "print('Pearson:', stats.pearsonr(pred, lbls)[0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7cef84383932084e6cb77734d7a8507a308d3db316a3c7126399eefad49f418"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('hackathons')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
