{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datalocation):\n",
    "        self.datalocation = datalocation\n",
    "        self.data = []\n",
    "        self.scores = []\n",
    "        self.vocab = set({'<unk>'})\n",
    "        self.word2Index = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
    "        self.index2Word = {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>'}\n",
    "        self.wordFrequency = dict()\n",
    "        print(\"Reading data...\")\n",
    "        self.read_data()\n",
    "        print(\"Preprocessing data...\")\n",
    "        self.preprocess()\n",
    "        print(\"Handling unknown tokens...\")\n",
    "        self.handle_unkown_token()\n",
    "        print(\"\")\n",
    "        \n",
    "\n",
    "    def read_data(self):\n",
    "        csvreader = csv.reader(open(self.datalocation, 'r'), delimiter='\\t')\n",
    "        for row in csvreader:\n",
    "            try:\n",
    "                self.data.append((row[5],row[6]))\n",
    "                self.scores.append(row[4])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "    def clean_data(self, tokenized_sentence):\n",
    "        cleaned_sentence = []\n",
    "        for token in tokenized_sentence:\n",
    "            if token not in string.punctuation:\n",
    "                cleaned_sentence.append(token)\n",
    "    \n",
    "        return cleaned_sentence\n",
    "    \n",
    "    def preprocess(self):\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = [tok.text for tok in spacy_en.tokenizer(self.data[idx][0].lower())]\n",
    "            s2 = [tok.text for tok in spacy_en.tokenizer(self.data[idx][1].lower())]\n",
    "            s1 = self.clean_data(s1)\n",
    "            s2 = self.clean_data(s2)\n",
    "            for token in s1:\n",
    "                if token not in self.wordFrequency:\n",
    "                    self.wordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.wordFrequency[token] += 1\n",
    "        \n",
    "            for token in s2:\n",
    "                if token not in self.wordFrequency:\n",
    "                    self.wordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.wordFrequency[token] += 1\n",
    "            \n",
    "            self.data[idx] = (s1, s2)\n",
    "\n",
    "        \n",
    "    def handle_unkown_token(self):\n",
    "        self.unkown_token = '<unk>'\n",
    "        self.word2Index[self.unkown_token] = len(self.word2Index)\n",
    "        self.index2Word[len(self.index2Word)] = self.unkown_token\n",
    "        print(self.data[0])\n",
    "\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "\n",
    "            for i in range(len(s1)):\n",
    "                word = s1[i]\n",
    "                if self.wordFrequency[word] < 2:\n",
    "                    s1[i] = self.unkown_token\n",
    "            \n",
    "            for i in range(len(s2)):\n",
    "                word = s2[i]\n",
    "                if self.wordFrequency[word] < 2:\n",
    "                    s2[i] = self.unkown_token\n",
    "            \n",
    "            self.data[idx] = (s1, s2)\n",
    "        \n",
    "        print(self.data[0])\n",
    "        \n",
    "        self.wordFrequency = dict()\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "\n",
    "            for token in s1:\n",
    "                self.vocab.add(token)\n",
    "                if token not in self.wordFrequency:\n",
    "                    self.word2Index[token] = len(self.word2Index)\n",
    "                    self.index2Word[len(self.index2Word)] = token\n",
    "                    self.wordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.wordFrequency[token] += 1\n",
    "        \n",
    "            for token in s2:\n",
    "                self.vocab.add(token)\n",
    "                if token not in self.wordFrequency:\n",
    "                    self.word2Index[token] = len(self.word2Index)\n",
    "                    self.index2Word[len(self.index2Word)] = token\n",
    "                    self.wordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.wordFrequency[token] += 1\n",
    "            \n",
    "            s1 = ['<sos>'] + s1 + ['<eos>']\n",
    "            s2 = ['<sos>'] + s2 + ['<eos>']\n",
    "\n",
    "            if len(s1) > len(s2):\n",
    "                s2 = s2 + ['<pad>'] * (len(s1) - len(s2))\n",
    "            elif len(s1) < len(s2):\n",
    "                s1 = s1 + ['<pad>'] * (len(s2) - len(s1))\n",
    "            \n",
    "            self.data[idx] = (s1, s2)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:00<00:00, 6781.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling unknown tokens...\n",
      "(['a', 'plane', 'is', 'taking', 'off'], ['an', 'air', 'plane', 'is', 'taking', 'off'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:00<00:00, 302794.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['a', 'plane', 'is', 'taking', 'off'], ['an', 'air', 'plane', 'is', 'taking', 'off'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:00<00:00, 145443.04it/s]\n"
     ]
    }
   ],
   "source": [
    "data = MyDataset('../stsbenchmark/sts-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7735 is out of bounds for axis 0 with size 7735",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4465/3616958015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2Index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mscore1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7735 is out of bounds for axis 0 with size 7735"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "df = dict()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    tokens = data[i][0]\n",
    "    for token in tokens:\n",
    "        if token in df:\n",
    "            df[token].add(i)\n",
    "        else:\n",
    "            df[token] = {i}\n",
    "    \n",
    "    tokens = data[i][1]\n",
    "    for token in tokens:\n",
    "        if token in df:\n",
    "            df[token].add(i)\n",
    "        else:\n",
    "            df[token] = {i}\n",
    "\n",
    "vocab_index = len(data.vocab)\n",
    "vocab = dict()\n",
    "\n",
    "for word in df:\n",
    "    vocab[word] = vocab_index\n",
    "    vocab_index += 1\n",
    "\n",
    "\n",
    "tf_idf = dict()\n",
    "\n",
    "tf_idf_scores = list()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    tokens = data[i][0]\n",
    "    scores = np.zeros(len(data.vocab))\n",
    "    counter = Counter(tokens)\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token]/len(tokens)\n",
    "        idf = math.log(len(data)/(len(df[token])+1))\n",
    "        scores[data.word2Index[token]] = tf*idf\n",
    "    score1 = scores \n",
    "\n",
    "    tokens = data[i][1]\n",
    "    scores = np.zeros(len(data.vocab))\n",
    "    counter = Counter(tokens)\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token]/len(tokens)\n",
    "        idf = math.log(len(data)/(len(df[token])+1))\n",
    "        scores[data.word2Index[token]] = tf*idf\n",
    "    score2 = scores\n",
    "\n",
    "    tf_idf_scores.append((score1,score2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f54ec556731191ae34ffecbd12704bb8f2e6a5cabce98c16cdcccd50acdfe5bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
