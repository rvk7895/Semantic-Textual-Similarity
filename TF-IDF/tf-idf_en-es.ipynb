{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk \n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_es = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_file, Y_file):\n",
    "        self.X_file = X_file\n",
    "        self.Y_file = Y_file\n",
    "        self.data = []\n",
    "        self.scores = []\n",
    "        self.ENvocab = set({'<unk>'})\n",
    "        self.ENword2Index = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
    "        self.ENindex2Word = {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>'}\n",
    "        self.ENwordFrequency = dict()\n",
    "        self.ESvocab = set({'<unk>'})\n",
    "        self.ESword2Index = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
    "        self.ESindex2Word = {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>'}\n",
    "        self.ESwordFrequency = dict()\n",
    "        print(\"Reading data...\")\n",
    "        self.read_data()\n",
    "        print(\"Preprocessing data...\")\n",
    "        self.preprocess()\n",
    "        print(\"Handling unknown tokens...\")\n",
    "        self.handle_unkown_token()\n",
    "        print(\"TF-IDF...\")\n",
    "        self.tf_idf()\n",
    "        \n",
    "\n",
    "    def read_data(self):\n",
    "        with open(self.X_file, 'r') as inFile:\n",
    "            data = inFile.readlines()\n",
    "            for line in data:\n",
    "                sentences = line.split('\\t')\n",
    "                self.data.append((sentences[0], sentences[1]))\n",
    "        \n",
    "        with open(self.Y_file, 'r') as inFile:\n",
    "            data = inFile.readlines()\n",
    "            for line in data:\n",
    "                self.scores.append(int(line))\n",
    "        \n",
    "    def clean_data(self, tokenized_sentence):\n",
    "        cleaned_sentence = []\n",
    "        for token in tokenized_sentence:\n",
    "            if token not in string.punctuation:\n",
    "                cleaned_sentence.append(token)\n",
    "    \n",
    "        return cleaned_sentence\n",
    "    \n",
    "    def preprocess(self):\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = [tok.text for tok in spacy_en.tokenizer(self.data[idx][0].lower())]\n",
    "            s2 = [tok.text for tok in spacy_es.tokenizer(self.data[idx][1].lower())]\n",
    "            s1 = self.clean_data(s1)\n",
    "            s2 = self.clean_data(s2)\n",
    "            for token in s1:\n",
    "                if token not in self.ENwordFrequency:\n",
    "                    self.ENwordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.ENwordFrequency[token] += 1\n",
    "        \n",
    "            for token in s2:\n",
    "                if token not in self.ESwordFrequency:\n",
    "                    self.ESwordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.ESwordFrequency[token] += 1\n",
    "            \n",
    "            self.data[idx] = (s1, s2)\n",
    "\n",
    "        \n",
    "    def handle_unkown_token(self):\n",
    "        self.unkown_token = '<unk>'\n",
    "        self.ENword2Index[self.unkown_token] = len(self.ENword2Index)\n",
    "        self.ENindex2Word[len(self.ENindex2Word)] = self.unkown_token\n",
    "\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "\n",
    "            for i in range(len(s1)):\n",
    "                word = s1[i]\n",
    "                if self.ENwordFrequency[word] < 2:\n",
    "                    s1[i] = self.unkown_token\n",
    "            \n",
    "            for i in range(len(s2)):\n",
    "                word = s2[i]\n",
    "                if self.ESwordFrequency[word] < 2:\n",
    "                    s2[i] = self.unkown_token\n",
    "            \n",
    "            self.data[idx] = (s1, s2)\n",
    "        \n",
    "        self.ENwordFrequency = dict()\n",
    "        self.ESwordFrequency = dict()\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "\n",
    "            for token in s1:\n",
    "                self.ENvocab.add(token)\n",
    "                if token not in self.ENwordFrequency:\n",
    "                    self.ENword2Index[token] = len(self.ENword2Index)\n",
    "                    self.ENindex2Word[len(self.ENindex2Word)] = token\n",
    "                    self.ENwordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.ENwordFrequency[token] += 1\n",
    "        \n",
    "            for token in s2:\n",
    "                self.ESvocab.add(token)\n",
    "                if token not in self.ESwordFrequency:\n",
    "                    self.ESword2Index[token] = len(self.ESword2Index)\n",
    "                    self.ESindex2Word[len(self.ESindex2Word)] = token\n",
    "                    self.ESwordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.ESwordFrequency[token] += 1\n",
    "            \n",
    "            s1 = ['<sos>'] + s1 + ['<eos>']\n",
    "            s2 = ['<sos>'] + s2 + ['<eos>']\n",
    "\n",
    "            if len(s1) > len(s2):\n",
    "                s2 = s2 + ['<pad>'] * (len(s1) - len(s2))\n",
    "            elif len(s1) < len(s2):\n",
    "                s1 = s1 + ['<pad>'] * (len(s2) - len(s1))\n",
    "            \n",
    "            self.data[idx] = (s1, s2)\n",
    "    \n",
    "    def tf_idf(self):\n",
    "        self.ENtf = dict()\n",
    "        self.EStf = dict()\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "            for token in s1:\n",
    "                if token not in self.ENtf:\n",
    "                    self.ENtf[token] = 1\n",
    "                else:\n",
    "                    self.ENtf[token] += 1\n",
    "            for token in s2:\n",
    "                if token not in self.EStf:\n",
    "                    self.EStf[token] = 1\n",
    "                else:\n",
    "                    self.EStf[token] += 1\n",
    "        \n",
    "        self.ENidf = dict()\n",
    "        self.ESidf = dict()\n",
    "        for token in self.ENtf:\n",
    "            self.ENidf[token] = np.log(len(self.data) / self.ENtf[token])\n",
    "        for token in self.EStf:\n",
    "            self.ESidf[token] = np.log(len(self.data) / self.EStf[token])\n",
    "        \n",
    "        self.tf_idf_data = []\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "            \n",
    "            s1_tf = np.zeros(len(self.ENword2Index))\n",
    "            s2_tf = np.zeros(len(self.ESword2Index))\n",
    "\n",
    "            for token in s1:\n",
    "                s1_tf[self.ENword2Index[token]] = self.ENidf[token]\n",
    "            \n",
    "            for token in s2:\n",
    "                s2_tf[self.ESword2Index[token]] = self.ESidf[token]\n",
    "            \n",
    "            self.tf_idf_data.append((s1_tf, s2_tf))\n",
    "\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tf_idf_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 8749.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling unknown tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 142184.62it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 64507.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 99681.63it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 52924.30it/s]\n"
     ]
    }
   ],
   "source": [
    "data = MyDataset('../sts-2017-en-es/En_Es_STS/STS.input.en-es.train.txt', '../sts-2017-en-es/En_Es_STS/STS.input.en-es.train_scores.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_datapoints = list()\n",
    "ES_datapoints = list()\n",
    "\n",
    "for datapoints in data:\n",
    "    EN_datapoints.append(datapoints[0])\n",
    "    ES_datapoints.append(datapoints[1])\n",
    "\n",
    "EN_datapoints = np.array(EN_datapoints)\n",
    "ES_datapoints = np.array(ES_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100) (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SVD = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "\n",
    "EN_datapoints = SVD.fit_transform(EN_datapoints)\n",
    "ES_datapoints = SVD.fit_transform(ES_datapoints)\n",
    "\n",
    "print(EN_datapoints.shape, ES_datapoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cosine_similarity(EN_datapoints, ES_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.73975468e-01  1.31007404e-01 -5.29191381e-02 ...  2.32895302e-02\n",
      "  -1.27527987e-04  2.68196235e-01]\n",
      " [ 1.56385824e-01  3.90073162e-02  1.61929209e-01 ...  1.59522662e-01\n",
      "   1.54551996e-01 -1.46604164e-01]\n",
      " [-8.79260781e-03 -4.14174265e-02  1.93935119e-01 ...  5.29394210e-02\n",
      "   4.72425191e-02 -2.89874204e-02]\n",
      " ...\n",
      " [ 1.68707508e-01  1.16773893e-01  2.39514214e-02 ...  5.64764024e-02\n",
      "   2.23232674e-01  5.18287400e-02]\n",
      " [ 1.93477796e-01 -1.87018781e-02  1.04777169e-01 ...  3.78248934e-02\n",
      "   9.40088059e-02 -4.76415428e-02]\n",
      " [ 1.15611257e-01  1.10393717e-01  1.67091786e-01 ... -5.82096903e-02\n",
      "  -4.65110682e-02  5.32544009e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_needed = list()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    normalized_score = (scores[i][i] + 1)/2 * 6 - 0.5\n",
    "    scores_needed.append(round(normalized_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0677572613808956, 0.032156854609399904)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(data.scores, scores_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.228\n",
      "0.10545618035159933\n",
      "0.1824099827500182\n",
      "0.17239523602744963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.16      0.14      0.15       161\n",
      "           3       0.24      0.87      0.38       228\n",
      "           4       0.13      0.08      0.10        65\n",
      "           5       0.50      0.00      0.00       534\n",
      "\n",
      "    accuracy                           0.23      1000\n",
      "   macro avg       0.17      0.18      0.11      1000\n",
      "weighted avg       0.36      0.23      0.12      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rvk7895/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rvk7895/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rvk7895/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rvk7895/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score , f1_score, recall_score , precision_score, classification_report\n",
    "\n",
    "print(accuracy_score(data.scores, scores_needed))\n",
    "print(f1_score(data.scores, scores_needed, average='macro'))\n",
    "print(recall_score(data.scores, scores_needed, average='macro'))\n",
    "print(precision_score(data.scores, scores_needed, average='macro'))\n",
    "print(classification_report(data.scores, scores_needed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
