{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_file_location, y_file_location):\n",
    "        self.x_file_location = x_file_location\n",
    "        self.y_file_locaiton = y_file_location\n",
    "        self.initialize_data()\n",
    "        self.modify()\n",
    "        self.combined_data = list()\n",
    "        self.combineData()\n",
    "\n",
    "    def initialize_data(self):\n",
    "        with open(self.x_file_location, \"r\") as inFile:\n",
    "            data = inFile.readlines()\n",
    "        \n",
    "        self.scores = list()\n",
    "        with open(self.y_file_locaiton, \"r\") as inFile:\n",
    "            self.scores = inFile.readlines()\n",
    "\n",
    "        self.scores = [int(score) for score in self.scores]\n",
    "        \n",
    "        enData = list()\n",
    "        esData = list()\n",
    "\n",
    "        for line in data:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            enData.append(line[0])\n",
    "            esData.append(line[1])\n",
    "        \n",
    "        en_tokenized_data = self.tokenizer(enData)\n",
    "        self.ENdataset = self.cleaner(en_tokenized_data)\n",
    "        (\n",
    "            self.ENword2Index,\n",
    "            self.ENindex2Word,\n",
    "            self.ENvocab_size,\n",
    "            self.ENvocab,\n",
    "            self.ENwordFrequency\n",
    "        ) = self.vocabBuilder(self.ENdataset)\n",
    "        self.ENwords = list()\n",
    "        for sentence in self.ENdataset:\n",
    "            for word in sentence:\n",
    "                self.ENwords.append(word)\n",
    "\n",
    "        self.ENwords_indexes = [self.ENword2Index[word] for word in self.ENwords]\n",
    "\n",
    "        fr_tokenized_data = self.tokenizer(esData)\n",
    "        self.ESdataset = self.cleaner(fr_tokenized_data)\n",
    "        (\n",
    "            self.ESword2Index,\n",
    "            self.ESindex2Word,\n",
    "            self.ESvocab_size,\n",
    "            self.ESvocab,\n",
    "            self.ESwordFrequency\n",
    "        ) = self.vocabBuilder(self.ESdataset)\n",
    "        self.ESwords = list()\n",
    "        for sentence in self.ESdataset:\n",
    "            for word in sentence:\n",
    "                self.ESwords.append(word)\n",
    "\n",
    "        self.ESwords_indexes = [self.ESword2Index[word] for word in self.ESwords]\n",
    "\n",
    "    def tokenizer(self,corpus):\n",
    "        \"\"\"\n",
    "            tokenizes the corpus\n",
    "            \n",
    "            Arguments:\n",
    "                corpus (list)\n",
    "\n",
    "            Returns:\n",
    "                tokenized corpus (list)\n",
    "        \"\"\"\n",
    "        hashtag_regex = \"#[a-zA-Z0-9]+\"\n",
    "        url_regex = \"((http|https)://)(www.)?[a-zA-Z0-9@:%._\\\\+~#?&//=]{2,256}\\\\.[a-z]{2,6}\\\\b([-a-zA-Z0-9@:%._\\\\+~#?&//=]*)\"\n",
    "        mention_regex = \"@\\w+\"\n",
    "\n",
    "        processed_corpus = list()\n",
    "\n",
    "        for tweet in corpus:\n",
    "            normalized_tweet = tweet.lower()\n",
    "            hashtag_removed_tweet = re.sub(hashtag_regex, \"<HASHTAG>\", normalized_tweet)\n",
    "            website_removed_tweet = re.sub(url_regex, \"<URL>\", hashtag_removed_tweet)\n",
    "            mention_removed_tweet = re.sub(\n",
    "                mention_regex, \"<MENTION>\", website_removed_tweet\n",
    "            )\n",
    "            punctuation_repeat_removed = re.sub(\n",
    "                r\"(\\W)(?=\\1)\", \"\", mention_removed_tweet\n",
    "            )\n",
    "            tokenized_tweet = punctuation_repeat_removed.split()\n",
    "\n",
    "            cleaned_tokenized_tweet = list()\n",
    "            for token in tokenized_tweet:\n",
    "                if token not in [\"<HASHTAG>\", \"<URL>\", \"<MENTION>\", \"<OOV>\"]:\n",
    "                    split_tokens = \"\".join(\n",
    "                        (char if char.isalpha() or char.isnumeric() else f\" {char} \")\n",
    "                        for char in token\n",
    "                    ).split()\n",
    "                    for cleaned_token in split_tokens:\n",
    "                        cleaned_tokenized_tweet.append(cleaned_token)\n",
    "\n",
    "                else:\n",
    "                    cleaned_tokenized_tweet.append(token)\n",
    "            cleaned_tokenized_tweet = ['<SOS>'] + cleaned_tokenized_tweet + ['<EOS>']\n",
    "            processed_corpus.append(cleaned_tokenized_tweet)\n",
    "\n",
    "        return processed_corpus\n",
    "\n",
    "    def cleaner(self,corpus):\n",
    "        \"\"\"\n",
    "            replacing !,?,. with . and removing other punctuations\n",
    "            \n",
    "            Arguments:\n",
    "                tokenized corpuse (list)\n",
    "\n",
    "            Returns:\n",
    "                cleaned corpus (list)\n",
    "        \"\"\"\n",
    "        import string\n",
    "\n",
    "        cleaned_corpus = list()\n",
    "\n",
    "        for sentence in corpus:\n",
    "            new_sentence = list()\n",
    "            for token in sentence:\n",
    "                if token in [\"!\", \".\", \"?\"]:\n",
    "                    new_sentence.append(\".\")\n",
    "                elif token in string.punctuation:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_sentence.append(token)\n",
    "\n",
    "            cleaned_corpus.append(new_sentence)\n",
    "\n",
    "        return cleaned_corpus\n",
    "\n",
    "    def vocabBuilder(self,corpus):\n",
    "        \"\"\"\n",
    "            Builds the vocabulary of the input dataset.\n",
    "\n",
    "            Arguments:\n",
    "                The cleaned tokenized the dataset\n",
    "            \n",
    "            Returns:\n",
    "                Word to Index dict, Index to Word list, Number of Unique Words, Set of Vocab\n",
    "        \"\"\"\n",
    "        word2Index = dict()\n",
    "        index2Word = list()\n",
    "        vocab = set()\n",
    "        wordFrequency = dict()\n",
    "\n",
    "        n_unique_words = 0\n",
    "\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                vocab.add(word)\n",
    "                if word not in word2Index:\n",
    "                    word2Index[word] = n_unique_words\n",
    "                    index2Word.append(word)\n",
    "                    n_unique_words += 1\n",
    "                    wordFrequency[word] = 1\n",
    "                else:\n",
    "                    wordFrequency[word] += 1\n",
    "\n",
    "        return word2Index, index2Word, n_unique_words, vocab, wordFrequency\n",
    "    \n",
    "    def modify(self):\n",
    "        for i in range(len(self.ENdataset)):\n",
    "            for j in range(len(self.ENdataset[i])):\n",
    "                if self.ENwordFrequency[self.ENdataset[i][j]] < 2:\n",
    "                    self.ENdataset[i][j] = '<OOV>'\n",
    "                elif any(character.isdigit() for character in self.ENdataset[i][j]):\n",
    "                    self.ENdataset[i][j] = '<OOV>'\n",
    "\n",
    "        print(self.ENvocab_size)\n",
    "        \n",
    "        self.ENdataset = self.cleaner(self.ENdataset)\n",
    "        (\n",
    "            self.ENword2Index,\n",
    "            self.ENindex2Word,\n",
    "            self.ENvocab_size,\n",
    "            self.ENvocab,\n",
    "            self.ENwordFrequency\n",
    "        ) = self.vocabBuilder(self.ENdataset)\n",
    "        self.ENwords = list()\n",
    "        for sentence in self.ENdataset:\n",
    "            for word in sentence:\n",
    "                self.ENwords.append(word)\n",
    "\n",
    "        self.ENwords_indexes = [self.ENword2Index[word] for word in self.ENwords]\n",
    "\n",
    "        for i in range(len(self.ESdataset)):\n",
    "            for j in range(len(self.ESdataset[i])):\n",
    "                if self.ESwordFrequency[self.ESdataset[i][j]] < 2:\n",
    "                    self.ESdataset[i][j] = '<OOV>'\n",
    "                elif any(character.isdigit() for character in self.ESdataset[i][j]):\n",
    "                    self.ESdataset[i][j] = '<OOV>'\n",
    "\n",
    "        self.ESdataset = self.cleaner(self.ESdataset)\n",
    "        (\n",
    "            self.ESword2Index,\n",
    "            self.ESindex2Word,\n",
    "            self.ESvocab_size,\n",
    "            self.ESvocab,\n",
    "            self.ESwordFrequency\n",
    "        ) = self.vocabBuilder(self.ESdataset)\n",
    "        self.ESwords = list()\n",
    "        for sentence in self.ESdataset:\n",
    "            for word in sentence:\n",
    "                self.ESwords.append(word)\n",
    "\n",
    "        self.ESwords_indexes = [self.ESword2Index[word] for word in self.ESwords]\n",
    "\n",
    "        print(self.ESvocab_size)\n",
    "\n",
    "    def combineData(self):\n",
    "        for idx in range(len(self.ENdataset)):\n",
    "            self.combined_data.append((self.ENdataset[idx], self.ESdataset[idx]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ESdataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            np.array(self.ENdataset[index]),\n",
    "            np.array(self.ESdataset[index]),\n",
    "            self.scores[index]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426\n",
      "2066\n"
     ]
    }
   ],
   "source": [
    "data = Dataset('../sts-2017-en-es/En_Es_STS/STS.input.en-es.train.txt', '../sts-2017-en-es/En_Es_STS/STS.input.en-es.train_scores.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'sequence_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18647/147142131.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_18647/1618748898.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         return (\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENwords_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESwords_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'sequence_length'"
     ]
    }
   ],
   "source": [
    "next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f54ec556731191ae34ffecbd12704bb8f2e6a5cabce98c16cdcccd50acdfe5bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
