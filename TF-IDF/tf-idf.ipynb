{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datalocation):\n",
    "        self.datalocation = datalocation\n",
    "        self.data = []\n",
    "        self.scores = []\n",
    "        self.vocab = set({'<unk>'})\n",
    "        self.word2Index = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
    "        self.index2Word = {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>'}\n",
    "        self.wordFrequency = dict()\n",
    "        print(\"Reading data...\")\n",
    "        self.read_data()\n",
    "        print(\"Preprocessing data...\")\n",
    "        self.preprocess()\n",
    "        print(\"Handling unknown tokens...\")\n",
    "        self.handle_unkown_token()\n",
    "        print(\"TF-IDF...\")\n",
    "        self.tf_idf()\n",
    "        \n",
    "\n",
    "    def read_data(self):\n",
    "        csvreader = csv.reader(open(self.datalocation, 'r'), delimiter='\\t')\n",
    "        for row in csvreader:\n",
    "            try:\n",
    "                self.data.append((row[5],row[6]))\n",
    "                self.scores.append(float(row[4]))\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "    def clean_data(self, tokenized_sentence):\n",
    "        cleaned_sentence = []\n",
    "        for token in tokenized_sentence:\n",
    "            if token not in string.punctuation:\n",
    "                cleaned_sentence.append(token)\n",
    "    \n",
    "        return cleaned_sentence\n",
    "    \n",
    "    def preprocess(self):\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = [tok.text for tok in spacy_en.tokenizer(self.data[idx][0].lower())]\n",
    "            s2 = [tok.text for tok in spacy_en.tokenizer(self.data[idx][1].lower())]\n",
    "            s1 = self.clean_data(s1)\n",
    "            s2 = self.clean_data(s2)\n",
    "            for token in s1:\n",
    "                if token not in self.wordFrequency:\n",
    "                    self.wordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.wordFrequency[token] += 1\n",
    "        \n",
    "            for token in s2:\n",
    "                if token not in self.wordFrequency:\n",
    "                    self.wordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.wordFrequency[token] += 1\n",
    "            \n",
    "            self.data[idx] = (s1, s2)\n",
    "\n",
    "        \n",
    "    def handle_unkown_token(self):\n",
    "        self.unkown_token = '<unk>'\n",
    "        self.word2Index[self.unkown_token] = len(self.word2Index)\n",
    "        self.index2Word[len(self.index2Word)] = self.unkown_token\n",
    "        print(self.data[0])\n",
    "\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "\n",
    "            for i in range(len(s1)):\n",
    "                word = s1[i]\n",
    "                if self.wordFrequency[word] < 2:\n",
    "                    s1[i] = self.unkown_token\n",
    "            \n",
    "            for i in range(len(s2)):\n",
    "                word = s2[i]\n",
    "                if self.wordFrequency[word] < 2:\n",
    "                    s2[i] = self.unkown_token\n",
    "            \n",
    "            self.data[idx] = (s1, s2)\n",
    "        \n",
    "        print(self.data[0])\n",
    "        \n",
    "        self.wordFrequency = dict()\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "\n",
    "            for token in s1:\n",
    "                self.vocab.add(token)\n",
    "                if token not in self.wordFrequency:\n",
    "                    self.word2Index[token] = len(self.word2Index)\n",
    "                    self.index2Word[len(self.index2Word)] = token\n",
    "                    self.wordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.wordFrequency[token] += 1\n",
    "        \n",
    "            for token in s2:\n",
    "                self.vocab.add(token)\n",
    "                if token not in self.wordFrequency:\n",
    "                    self.word2Index[token] = len(self.word2Index)\n",
    "                    self.index2Word[len(self.index2Word)] = token\n",
    "                    self.wordFrequency[token] = 1\n",
    "                else:\n",
    "                    self.wordFrequency[token] += 1\n",
    "            \n",
    "            s1 = ['<sos>'] + s1 + ['<eos>']\n",
    "            s2 = ['<sos>'] + s2 + ['<eos>']\n",
    "\n",
    "            if len(s1) > len(s2):\n",
    "                s2 = s2 + ['<pad>'] * (len(s1) - len(s2))\n",
    "            elif len(s1) < len(s2):\n",
    "                s1 = s1 + ['<pad>'] * (len(s2) - len(s1))\n",
    "            \n",
    "            self.data[idx] = (s1, s2)\n",
    "    \n",
    "    def tf_idf(self):\n",
    "        self.df = dict()\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "            for token in s1:\n",
    "                if token not in self.df:\n",
    "                    self.df[token] = 1\n",
    "                else:\n",
    "                    self.df[token] += 1\n",
    "            for token in s2:\n",
    "                if token not in self.df:\n",
    "                    self.df[token] = 1\n",
    "                else:\n",
    "                    self.df[token] += 1\n",
    "        \n",
    "        self.idf = dict()\n",
    "        for token in self.df:\n",
    "            self.idf[token] = math.log(len(self.data) / self.df[token])\n",
    "        \n",
    "        self.tf_idf_data = []\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            s1 = self.data[idx][0]\n",
    "            s2 = self.data[idx][1]\n",
    "            tf_idf_s1 = np.zeros(len(self.word2Index))\n",
    "            tf_idf_s2 = np.zeros(len(self.word2Index))\n",
    "\n",
    "            for token in s1:\n",
    "                tf_idf_s1[self.word2Index[token]] = self.idf[token]\n",
    "            for token in s2:\n",
    "                tf_idf_s2[self.word2Index[token]] = self.idf[token]\n",
    "            \n",
    "            self.tf_idf_data.append((tf_idf_s1, tf_idf_s2))\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tf_idf_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:01<00:00, 5499.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling unknown tokens...\n",
      "(['a', 'plane', 'is', 'taking', 'off'], ['an', 'air', 'plane', 'is', 'taking', 'off'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:00<00:00, 264849.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['a', 'plane', 'is', 'taking', 'off'], ['an', 'air', 'plane', 'is', 'taking', 'off'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:00<00:00, 120054.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:00<00:00, 162989.83it/s]\n",
      "100%|██████████| 5708/5708 [00:00<00:00, 17163.83it/s]\n"
     ]
    }
   ],
   "source": [
    "data = MyDataset('../stsbenchmark/sts-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x1, x2):\n",
    "    return -1 * cosine_similarity(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:00<00:00, 14004.35it/s]\n"
     ]
    }
   ],
   "source": [
    "sentencesL = []\n",
    "sentencesR = []\n",
    "scores = []\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    sentencesL.append(np.array(data[i][0]))\n",
    "    sentencesR.append(np.array(data[i][1]))\n",
    "\n",
    "sentencesL = np.array(sentencesL)\n",
    "sentencesR = np.array(sentencesR)\n",
    "\n",
    "scores = cosine_similarity(sentencesL, sentencesR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_needed = []\n",
    "for i in range(len(data)):\n",
    "    scores_needed.append(scores[i][i] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6946474539889476, 0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(data.scores, scores_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f54ec556731191ae34ffecbd12704bb8f2e6a5cabce98c16cdcccd50acdfe5bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
